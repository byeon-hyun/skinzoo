{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3b94ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np \n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping\n",
    "from IPython.display import Image, display\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.layers import Dense, Input, Embedding, Dropout, concatenate,Conv1D, GlobalMaxPooling1D,Concatenate\n",
    "import PIL\n",
    "from PIL import ImageFile\n",
    "from PIL import ImageOps\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce14692b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = 'C:/Users/oceanlightai/Desktop/datasets/pet_skin/train/train_image3'\n",
    "target_dir = 'C:/Users/oceanlightai/Desktop/datasets/pet_skin/train/train_mask3'\n",
    "img_size = (256,256)\n",
    "num_classes = 6\n",
    "batch_size = 32\n",
    "MAX_WORDS = 10000  \n",
    "SEQUENCE_LENGTH = 1  \n",
    "BREED_CLASSES = 50  \n",
    "LESIONS_CLASSES = 6\n",
    "LOCATION_CLASSES = 4\n",
    "\n",
    "input_img_paths = sorted([os.path.join(input_dir,fname)\n",
    "                         for fname in os.listdir(input_dir)\n",
    "                         if fname.endswith('.jpg')])\n",
    "\n",
    "target_img_paths = sorted([os.path.join(target_dir,fname)\n",
    "                         for fname in os.listdir(target_dir)\n",
    "                         if fname.endswith('.jpg') and not fname.startswith('.')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202742ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SkinDiseaseDataset(keras.utils.Sequence):\n",
    "    def __init__(self, batch_size, img_size, input_img_paths, target_img_paths, text_data):\n",
    "        # text_data 추가\n",
    "        self.batch_size = batch_size\n",
    "        self.img_size = img_size\n",
    "        self.input_img_paths = input_img_paths\n",
    "        self.target_img_paths = target_img_paths\n",
    "        self.text_data = text_data\n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.target_img_paths) // self.batch_size\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        i = idx * self.batch_size\n",
    "        batch_input_img_paths = self.input_img_paths[i:i+self.batch_size]\n",
    "        batch_target_img_paths = self.target_img_paths[i:i+self.batch_size]\n",
    "        batch_text_data = self.text_data[i:i+self.batch_size]  # text_data 처리\n",
    "        for j, path in enumerate(batch_input_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size)\n",
    "            x[j] = img_to_array(img) / 255.  # input normalization\n",
    "        y = np.zeros((self.batch_size,) + self.img_size + (1,), dtype='uint8')\n",
    "        for j, path in enumerate(batch_target_img_paths):\n",
    "            img = load_img(path, target_size=self.img_size, color_mode='grayscale')\n",
    "            img = img_to_array(img) // 51  # 255 / 5 = 51으로 나누면 0~5 범위를 얻을 수 있습니다.\n",
    "            y[j] = img\n",
    "        return [x, batch_text_data], y  # 반환 값 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e975133",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_texts(texts):\n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "    return pad_sequences(sequences, maxlen=SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce54b711",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88ba011",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "your_dataframe = pd.read_csv('C:/Users/oceanlightai/Desktop/datasets/pet_skin/train/train_skin.csv')\n",
    "\n",
    "# Remove 'Raw data ID' and 'polygon_location' columns\n",
    "your_dataframe = your_dataframe.drop(columns=['Raw data ID', 'polygon_location'])\n",
    "\n",
    "# Apply LabelEncoder to each categorical variable\n",
    "le = LabelEncoder()\n",
    "your_dataframe['breed'] = le.fit_transform(your_dataframe['breed'])\n",
    "your_dataframe['gender'] = le.fit_transform(your_dataframe['gender'])\n",
    "your_dataframe['region'] = le.fit_transform(your_dataframe['region'])\n",
    "\n",
    "# Define the model\n",
    "model = DecisionTreeClassifier()\n",
    "\n",
    "# Prepare the data (X: features, y: target 'lesions')\n",
    "X = your_dataframe.drop(['lesions'], axis=1)\n",
    "y = your_dataframe['lesions']\n",
    "\n",
    "# Train the model\n",
    "model.fit(X, y)\n",
    "\n",
    "# Print feature importances\n",
    "feature_importances = model.feature_importances_\n",
    "for name, importance in zip(X.columns, feature_importances):\n",
    "    print(f'Feature: {name}, Importance: {importance}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def24b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, image_base_path):\n",
    "    image_data = process_images(data['Raw data ID'], image_base_path)\n",
    "    age_data = data['age'].values\n",
    "\n",
    "    breed_labels = to_categorical(label_encoder.fit_transform(data['breed']))\n",
    "    lesions_labels = to_categorical(label_encoder.fit_transform(data['lesions']))\n",
    "    region_labels = to_categorical(label_encoder.fit_transform(data['region']))\n",
    "\n",
    "    return image_data, age_data,  lesions_labels, region_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1253cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unet(img_size, num_classes):\n",
    "    inputs = keras.Input(shape=img_size + (3,))\n",
    "    \n",
    "    x = layers.Conv2D(32, 3, strides=2, padding='same')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation('relu')(x)\n",
    "    \n",
    "    previous_block_activation = x \n",
    "    \n",
    "    for filters in [64, 128, 256]:\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.SeparableConv2D(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.MaxPooling2D(3, strides=2, padding='same')(x)\n",
    "        \n",
    "        residual = layers.Conv2D(filters, 1, strides=2, padding='same')(previous_block_activation)\n",
    "        x = layers.add([x, residual])\n",
    "        previous_block_activation = x\n",
    "        \n",
    "    for filters in [256, 128, 64, 32]:\n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.Activation('relu')(x)\n",
    "        x = layers.Conv2DTranspose(filters, 3, padding='same')(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        \n",
    "        x = layers.UpSampling2D(2)(x)\n",
    "        \n",
    "        residual = layers.UpSampling2D(2)(previous_block_activation)\n",
    "        residual = layers.Conv2D(filters, 1, padding='same')(residual)\n",
    "        x = layers.add([x, residual])\n",
    "        previous_block_activation = x \n",
    "        \n",
    "    outputs = layers.Conv2D(num_classes, 3, activation='relu', padding='same')(x)  # activation을 'relu'로 변경\n",
    "    x = layers.GlobalMaxPooling2D()(outputs)  # GlobalMaxPooling2D 추가\n",
    "    model = keras.Model(inputs, x)  # outputs 대신 x를 사용\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131a7fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f451744",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = Input(shape=(1,), dtype='int32')\n",
    "# 범주 임베딩\n",
    "x = Embedding(input_dim=7, output_dim=EMBEDDING_DIM)(input_text)\n",
    "x = Dense(128, activation='relu')(input_text)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.4)(x)\n",
    "text_model = Model(inputs=input_text, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e4a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_model = get_unet((256, 256), num_classes=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e288495a",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_input = concatenate([unet_model.output, text_model.output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1dddda9",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_lesions = Dense(LESIONS_CLASSES, activation='softmax', name='lesions')(combined_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebfcc796",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(inputs=[unet_model.input, text_model.input], outputs=[output_lesions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4521484f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc20db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "model.compile(optimizer=Adam(learning_rate=1e-6), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "callbacks = [keras.callbacks.ModelCheckpoint('Unet_segmentation_1.h5', save_best_only=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d29b4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1개\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-gpu",
   "language": "python",
   "name": "gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
